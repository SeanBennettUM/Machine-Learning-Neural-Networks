{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUUS3RT0gfjr",
        "outputId": "464f6133-ef11-414d-8d54-6872104e0a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MNIST...\n",
            "\n",
            "Training Logistic Regression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load MNIST dataset\n",
        "print(\"Loading MNIST...\")\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "X = X / 255.0\n",
        "\n",
        "# Train-test split (60k train, 10k test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=10000, random_state=42\n",
        ")\n",
        "\n",
        "# PART A: Logistic Regression\n",
        "print(\"\\nTraining Logistic Regression...\")\n",
        "log_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Get accuracies\n",
        "train_acc_log = log_reg.score(X_train, y_train)\n",
        "test_acc_log = log_reg.score(X_test, y_test)\n",
        "\n",
        "print(f\"Logistic Regression - Train: {train_acc_log:.4f}, Test: {test_acc_log:.4f}\")\n",
        "\n",
        "# PART A & B: k-NN with different k values\n",
        "print(\"\\nTesting k-NN with different k values...\")\n",
        "k_values = [1, 3, 5, 7, 10]\n",
        "train_acc_knn = []\n",
        "test_acc_knn = []\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"Testing k={k}...\")\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    train_acc = knn.score(X_train, y_train)\n",
        "    test_acc = knn.score(X_test, y_test)\n",
        "\n",
        "    train_acc_knn.append(train_acc)\n",
        "    test_acc_knn.append(test_acc)\n",
        "\n",
        "    print(f\"k={k}: Train={train_acc:.4f}, Test={test_acc:.4f}\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot 1: k-NN accuracy vs k\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(k_values, train_acc_knn, 'o-', label=\"Train Accuracy\")\n",
        "plt.plot(k_values, test_acc_knn, 's-', label=\"Test Accuracy\")\n",
        "plt.xlabel(\"k (neighbors)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"k-NN Accuracy vs k\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 2: Model comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "models = ['Logistic Regression', f'Best k-NN (k={k_values[np.argmax(test_acc_knn)]})']\n",
        "test_scores = [test_acc_log, max(test_acc_knn)]\n",
        "plt.bar(models, test_scores)\n",
        "plt.ylabel(\"Test Accuracy\")\n",
        "plt.title(\"Model Comparison\")\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Add values on bars\n",
        "for i, v in enumerate(test_scores):\n",
        "    plt.text(i, v + 0.005, f'{v:.3f}', ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary\n",
        "print(f\"\\nSUMMARY:\")\n",
        "print(f\"Best Logistic Regression: {test_acc_log:.4f}\")\n",
        "print(f\"Best k-NN: {max(test_acc_knn):.4f} (k={k_values[np.argmax(test_acc_knn)]})\")\n",
        "\n",
        "# Bias-Variance explanation\n",
        "print(f\"\\nBias-Variance Trade-off:\")\n",
        "print(f\"k=1: Train={train_acc_knn[0]:.4f}, Test={test_acc_knn[0]:.4f} (High Variance)\")\n",
        "print(f\"k=10: Train={train_acc_knn[-1]:.4f}, Test={test_acc_knn[-1]:.4f} (High Bias)\")"
      ]
    }
  ]
}